{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data from S3 to Pandas via Athena\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "## Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Reading from Redshift](#Reading-from-Redshift)\n",
    "1. [Upload to S3](#Upload-to-S3)\n",
    "1. [Writing back to Redshift](#Writing-back-to-Redshift)\n",
    "\n",
    "\n",
    "\n",
    "## Introduction\n",
    "In this notebook we illustrate how to copy data from Redshift to S3 and vice-versa.\n",
    "\n",
    "### Prerequisites\n",
    "In order to successfully run this notebook, you'll first need to:\n",
    "1. Have a Redshift cluster within the same VPC.\n",
    "1. Preload that cluster with data from the [iris data set](https://archive.ics.uci.edu/ml/datasets/iris) in a table named public.irisdata.\n",
    "1. Update the credential file (`redshift_creds_template.json.nogit`) file with the appropriate information.\n",
    "\n",
    "Also, note that this Notebook instance needs to resolve to a private IP when connecting to the Redshift instance. There are two ways to resolve the Redshift DNS name to a private IP:\n",
    "1. The Redshift cluster is not publicly accessible so by default it will resolve to private IP.\n",
    "1. The Redshift cluster is publicly accessible and has an EIP associated with it but when accessed from within a VPC, it should resolve to private IP of the Redshift cluster. This is possible by setting following two VPC attributes to yes: DNS resolution and DNS hostnames. For instructions on setting that up, see Redshift public docs on [Managing Clusters in an Amazon Virtual Private Cloud (VPC)](https://docs.aws.amazon.com/redshift/latest/mgmt/managing-clusters-vpc.html).\n",
    "\n",
    "### Notebook Setup\n",
    "Let's start by installing `PyAthenaJDBC`, a python wrapper for Athena JDBC driver, adding a few imports and specifying a few configs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading pip-9.0.3-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 857kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 9.0.1\n",
      "    Uninstalling pip-9.0.1:\n",
      "      Successfully uninstalled pip-9.0.1\n",
      "Successfully installed pip-9.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyAthenaJDBC\n",
      "  Using cached PyAthenaJDBC-1.3.3-py2.py3-none-any.whl\n",
      "Collecting future (from PyAthenaJDBC)\n",
      "  Downloading future-0.16.0.tar.gz (824kB)\n",
      "\u001b[K    100% |████████████████████████████████| 829kB 1.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: botocore>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from PyAthenaJDBC)\n",
      "Collecting jpype1>=0.6.0 (from PyAthenaJDBC)\n",
      "  Using cached JPype1-0.6.2.tar.gz\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.0.0->PyAthenaJDBC)\n",
      "Requirement already satisfied: docutils>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.0.0->PyAthenaJDBC)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.0.0->PyAthenaJDBC)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.0.0->PyAthenaJDBC)\n",
      "Building wheels for collected packages: future, jpype1\n",
      "  Running setup.py bdist_wheel for future ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/c2/50/7c/0d83b4baac4f63ff7a765bd16390d2ab43c93587fac9d6017a\n",
      "  Running setup.py bdist_wheel for jpype1 ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/8e/f3/e6/a1250b8e8d2bd105f4dd21b1dc801dbcf5d815592443bfe741\n",
      "Successfully built future jpype1\n",
      "Installing collected packages: future, jpype1, PyAthenaJDBC\n",
      "Successfully installed PyAthenaJDBC-1.3.3 future-0.16.0 jpype1-0.6.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install PyAthenaJDBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from pyathenajdbc import connect\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "bucket='ar-sm-bucket' # put your s3 bucket name here, and create s3 bucket\n",
    "prefix = 'sagemaker/DEMO-athena'\n",
    "# customize to your bucket where you have stored the data\n",
    "\n",
    "credfile = 'athena_creds.properties'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from Athena\n",
    "We store the information needed to connect to Athena in a credentials file. See the file `athena_creds.properties` for an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "# Sample query for testing\n",
    "query = 'select * from elb_logs limit 5;'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a connection to athena using our credentials, and use this to query athena and store the result in a pandas DataFrame, which we then save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from Athena...\n",
      "Saving file\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading from Athena...\")\n",
    "\n",
    "def get_conn(credfile): \n",
    "    conn = connect(s3_staging_dir='s3://ar-sm-bucket/',credential_file=credfile,schema_name='sampledb', region_name=region)\n",
    "    return conn\n",
    "\n",
    "def get_df(credfile, query):\n",
    "    conn = get_conn(credfile)       \n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(query)\n",
    "        result_set = cur.fetchall()\n",
    "        colnames = [desc[0] for desc in cur.description]\n",
    "        df = pd.DataFrame.from_records(result_set, columns=colnames)\n",
    "    return df\n",
    "\n",
    "df = get_df(credfile, query)\n",
    "\n",
    "print(\"Saving file\")\n",
    "localFile = 'elb_logs.csv'\n",
    "df.to_csv(localFile, index=False)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Writing to S3...\")\n",
    "\n",
    "fObj = open(localFile, 'rb')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, localFile)).upload_fileobj(fObj)\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
